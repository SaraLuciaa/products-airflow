# ğŸ›’ AnÃ¡lisis EDA de Datos de Retail con Apache Airflow

Proyecto de **AnÃ¡lisis Exploratorio de Datos (EDA)** para datos de retail utilizando Apache Airflow y PySpark. El sistema procesa archivos CSV de productos, categorÃ­as y transacciones para generar reportes estadÃ­sticos completos.

---

## ğŸ”§ Requisitos

- Docker Desktop instalado y corriendo
- Docker Compose
- 8GB de RAM disponible (mÃ­nimo)
- Puertos disponibles: 8082 (Airflow Web), 5432 (PostgreSQL)

---

## ğŸ“ Estructura del Proyecto

```
airflow-SaraLuciaaa/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ tienda_etl_dag.py           # DAG anÃ¡lisis Productos y CategorÃ­as
â”‚   â””â”€â”€ transacciones_eda_dag.py    # DAG anÃ¡lisis Transacciones
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ Products/
â”‚   â”‚   â”œâ”€â”€ Categories.csv          # CatÃ¡logo de categorÃ­as
â”‚   â”‚   â””â”€â”€ ProductCategory.csv     # RelaciÃ³n producto-categorÃ­a
â”‚   â””â”€â”€ Transactions/
â”‚       â”œâ”€â”€ 102_Tran.csv           # Transacciones tienda 102
â”‚       â”œâ”€â”€ 103_Tran.csv           # Transacciones tienda 103
â”‚       â”œâ”€â”€ 107_Tran.csv           # Transacciones tienda 107
â”‚       â””â”€â”€ 110_Tran.csv           # Transacciones tienda 110
â”œâ”€â”€ reports/                        # Reportes generados automÃ¡ticamente
â”œâ”€â”€ logs/                          # Logs de Airflow
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ env.compose
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### 1. Clonar el Repositorio

```bash
git clone https://github.com/SaraLuciaa/products-airflow.git
cd products-airflow
```

### 2. Construir la Imagen de Docker

```bash
docker-compose --env-file env.compose build
```

Este proceso puede tardar varios minutos la primera vez, ya que instala todas las dependencias necesarias (Apache Airflow, PySpark, pandas, openpyxl, etc.).

### 3. Levantar los Contenedores

```bash
docker-compose --env-file env.compose up -d
```

Esto iniciarÃ¡ los siguientes servicios:
- **Airflow Webserver** (Puerto 8082)
- **Airflow Scheduler**
- **PostgreSQL** (Base de datos de metadatos)
- **Airflow Init** (InicializaciÃ³n)

### 4. Acceder a la Interfaz Web de Airflow

1. Abrir el navegador en: **http://localhost:8082**
2. Credenciales de acceso:
   - **Usuario:** `admin`
   - **ContraseÃ±a:** `admin`

### 5. Ejecutar los DAGs

Una vez dentro de la interfaz:

1. **Activar el DAG** usando el toggle (switch) a la izquierda
2. **Ejecutar el DAG** haciendo clic en el botÃ³n "Play" â–¶ï¸ (Trigger DAG)
3. **Monitorear el progreso** en la vista de Grid o Graph
4. **Ver logs** haciendo clic en cada tarea

### 6. Acceder a los Reportes

Los reportes se generan automÃ¡ticamente en la carpeta `reports/`:

```bash
# Ver reportes generados
ls reports/
```

TambiÃ©n puedes acceder desde el contenedor:

```bash
docker exec -it airflow-saraluciaaa-copia-webserver-1 ls /opt/airflow/reports
```

### 7. Detener los Contenedores

```bash
docker-compose down
```

Para eliminar tambiÃ©n los volÃºmenes:

```bash
docker-compose down -v
```